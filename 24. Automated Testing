Networking professionals often overlook the need for testing due to the tedious nature of this task. It takes significant time, because of the lack of tools to perform testing beyond doing it manually device by device. Also, testing configurations before pushing changes to a production environment requires some sort of lab or nonproduction environment. This setup can be very expensive simply because of the cost of the hardware Rackspace, power, and the like. Automation enables a much easier, more effective, and more efficient means to test a network infrastructure. These systems may be in the form of Python scripts or tools that are used to test connectivity among physical devices, or it can be in the form of new tools that enable more efficient access to equipment such as Cisco’s Virtual Internet Routing Lab and Cisco’s DevNet resources

Network test infrastructure:
    When you think of a network lab, more likely than not, you’re thinking of a data center that has network devices that are racked, stacked, and cabled together. In an ideal world, your lab network resembles the production network. Due to costs associated with physical space, power, cooling, and of network hardware, this setup is usually not a possibility. In fact, many organizations do not have any lab devices to test with. How unfortunate is that? Imagine if applications were deployed without proper test infrastructure. This situation would be a nightmare.

    Applications have become much more portable in the past two decades having migrated from bare-metal machines to virtual machines and now to containers. Virtual machines (and containers) changed the way application environments are built and tested. Full environments can be encapsulated, cloned, and replicated, which is exactly the type of infrastructure that is needed for network infrastructure testing

    Labs and testing network equipment cannot rely solely on hardware. While hardware will always have a place for testing. As an example: performance and throughput tests. Most testing is functional, especially as it pertains to network automation. When it comes to testing network automation, all that’s really needed is a software solution that exposes device APIs. What you need are low-cost alternatives for extensive testing that allows network engineers to test and certify new features. These alternatives must also be able to test the various types of APIs that exist on network devices. Also, without having to purchase hardware that requires space, power, and cooling

    Especially as you start testing network APIs on devices and SDN controllers, it does not make sense to deploy hardware first anymore. It’s too time consuming and costly especially when all you need is API access on the devices. When testing hardware performance, you may need high density 10G/100G ports; however, network automation testing has no such requirement

    There are three ways to test Cisco infrastructure without using hardware:
        Network Functions Virtualization — This system is the equivalent to the setup being used for this course
        Cisco Virtual Internet Routing Lab (VIRL) — A test platform offered by Cisco allows network engineers to drag and drop topologies simplifying the process even further for creating a lab environment
        DevNet Sandboxes — As you will see later in this lesson. The Cisco DevNet offers free and hosted devices and labs to test new APIs and products

Network function virtualization (NFV):
    Over the past few years, the industry has been undergoing significant change with how networks are built and managed. One of these such changes (and trends) is Network Functions Virtualization. NFV is about decoupling a network service from hardware and making it available as software either as an application, service, container, or virtual machine. It does not have to be a virtual machine, although that is the most common form factor of devices being deployed. Typically, you would use x86 servers such as the Cisco UCS platform. Many virtual machines from Cisco can be run on both on and off-premises in public cloud environments. For example, you can deploy Cisco CSR 1000V routers directly in AWS and have them connect back into the corporate network using to site to site VPN tunnels. In this case, you would effectively have a branch that is an Amazon virtual private cloud. Using software and NFV for testing puts the focus back on testing and minimizes the amount of time that is required for racking, stacking, and cabling

    Over the past few years, Cisco has been releasing virtual appliances for key network operating systems and controllers. For example, Cisco now offers virtual firewalls, virtual wireless LAN controllers, and SDN controllers. In fact, all devices being used in this course are virtual machines. Each network device and SDN controller that is used are all running as virtual machines. Not all virtual machines are meant for production, though. This feature is a key point to understand as you start using software-based networking solutions for testing. For example, NX-OSv and the ACI emulator are appliances that are meant specifically for testing, while ASAv and the CSR 1000V are production-grade virtual appliances supported by Cisco. Always check the data sheets as sometimes there are hardware-centric features that are not supported in virtual editions (even though they are running the same network operating system)

VIRL:
    Cisco’s Virtual Internet Routing Lab, better known as VIRL, is a powerful network simulation platform. It is meant to be a flexible, all-in-one virtual networking lab. This setup means that there is no need for bulky network equipment and hours of wiring. Also, it is easy to extend your virtual lab in VIRL to the physical world. A few key benefits of VIRL include the following:
       Cisco virtual machines running the same network operating systems as used in Cisco’s physical routers and switches. VIRL comes with a complete set of legal and licensed Cisco images with new OS releases provided regularly

       Powerful GUI for network design and simulation control

   VIRL supports various platforms, and from time to time, Cisco releases updates and new virtual platforms. VIRL also supports third-party VMs, which are typically Linux VMs, but other vendor devices can be run in VIRL as well. In recent days, there has been integration with GitHub providing you with very useful resources for getting started, managing projects, and using VIRL features. Over a dozen third-party VMs listed here: https://learningnetwork.cisco.com/docs/DOC-30476

   In VIRL, the virtual machines run the network operating system but are not representations of a particular hardware platform. This setup means that though features and configurations are the same between the virtual and physical platforms. There are limitations in VIRL regarding data plane activity. Also, virtual machines have no fans, no switch fabric, and no ASIC models. This characteristic is a great advantage as features and the actual network operating system are available in virtual form. However, there is also an inability to test certain attributes, such as throughput

   VIRL is built on top of Openstack and is used to orchestrate the virtual machines, building the topology and managing images. Openstack is normally known as a free and open-source platform for cloud computing, but in very recent days it has been leveraged for providing a virtual infrastructure for various hardware recourses including compute, networking and storage. Openstack uses a web-based dashboard and REST APIs which make Openstack programmable and customizable. This feature is the core of what enables the VIRL network simulation. Openstack provides the foundation for VIRL to be more than a basic network simulation program; instead, it provides the basis for full network virtualization and orchestration

   The VIRL topology service director is used to create the virtual machine, and then the virtual machines are linked (and therefore communicate) with one another using Openstack. In fact, Openstack allows communication not only among virtual machines, but among several of computers running VIRL in a cluster. This communication can be done in a cloud-based hosted platform or locally using Vmware ESXi or on bare-metal servers.

           
